{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 30%; float: right; margin: 10px; margin-right: 5%;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/FHNW_Logo.svg/2560px-FHNW_Logo.svg.png\" width=\"500\" style=\"float: left; filter: invert(50%);\"/>\n",
    "</div>\n",
    "\n",
    "<h1 style=\"text-align: left; margin-top: 10px; float: left; width: 60%;\">\n",
    "    Projekt Soziale Netzwerke Analysieren:<br> Soziale Himmelskrieger\n",
    "</h1>\n",
    "\n",
    "<p style=\"clear: both; text-align: left;\">\n",
    "    Bearbeitet durch Flurina Riner, Patrick Schürmann, Si Ben Tran im HS 2023.<br>Bachelor of Science FHNW in Data Science.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Setup & Libraries\n",
    "\n",
    "Hier in diesem Abschnitt importieren wir die wichtigsten Libraries für unser Projekt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Explorative Datenanalyse\n",
    "\n",
    "Bewertungskriterien:   \n",
    "Explorative Analysen der verwendeten Daten (Missing Values, Verteilungen, Plausilitätsprüfungen), allfällige Transformationen und Bereinigen oder Ausschliessen von fehlerhaften Daten. Wurde  geprüft, ob die Daten mit den Quelldaten übereinstimmen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Daten einlesen\n",
    "\n",
    "Der Nachfolgende Code Abschnitt liest die strukturierten Kaggle Daten aus JSON-Dateien, extrahiert die Werte und speichert diese in ein Pandas DataFarme ab. Diese Vorangehensweise erlaubt es uns mittels Pandas die Daten besser zu manipulieren und zu analysieren.\n",
    "Anschliessend erstellen wir einen Graphen Netzwerke für jede Episode und können somit die Interaktion zwischen den Charakteren visualisieren. Dies macht es für uns einfacher, Muster oder Beziehungen zu erkennen. Am Ende der Schleife wird `lst_every_episode` eine Liste von Tupeln mit Datenframes für jeden Charakter (Nodes) und Kante (Edges) enthalten, während `graph_every_episode` eine Liste von NetworkX-Graphen für jede Episode enthält. Dies ermöglicht es, die Interaktionen zwischen Charakteren in den verschiedenen \"Star Wars\"-Episoden zu analysieren und darzustellen und dient als Grundlage für die weitere Analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_every_episode = []\n",
    "graph_every_episode = []\n",
    "\n",
    "for episode in range(1, 8):\n",
    "    filename = f'data/Star Wars/starwars-episode-{episode}-interactions-allCharacters.json'\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        nodes = data['nodes']\n",
    "        df_nodes = pd.DataFrame(nodes)\n",
    "        df_nodes = df_nodes.reset_index(drop=False)\n",
    "        df_nodes = df_nodes.rename(columns={\"index\": \"ID\"})\n",
    "        edges = data['links']\n",
    "        df_edges = pd.DataFrame(edges)\n",
    "        lst_every_episode.append((df_nodes, df_edges))\n",
    "        G = nx.Graph()\n",
    "        for idx, row in df_nodes.iterrows():\n",
    "            G.add_node(row['ID'], label=row['name'], size=row['value'], color=row['colour'])\n",
    "        for idx, row in df_edges.iterrows():\n",
    "            G.add_edge(row['source'], row['target'], weight=row['value'])\n",
    "        graph_every_episode.append(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Fehlende Werte\n",
    "Wir arbeiten mit einem Datensatz von Kaggle und gehen nach unserer Lektüre des Beschriebs des Datensatzes davon aus, dass dieser keine fehlenden Werte haben sollte. Wir überprüfen dies im Code unten und können bestätigen, dass keine der Episoden fehlende Werte aufweist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode, (df_nodes, df_edges) in enumerate(lst_every_episode, start=1):\n",
    "\n",
    "    missing_values_nodes = df_nodes.isnull().sum()\n",
    "    print(f\"\\nEpisode {episode} - Fehlende Werte für Knoten:\")\n",
    "    print(missing_values_nodes)\n",
    "\n",
    "    missing_values_edges = df_edges.isnull().sum()\n",
    "    print(f\"\\nEpisode {episode} - Fehlende Werte für Kanten:\")\n",
    "    print(missing_values_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Verteilungen der Daten\n",
    "Wir schauen uns an, wie sich die Anzahl Kanten der Knoten verteilt. Zu prüfen, wie sich die Verteilung verhält, ist vor allem dann spannend, wenn zu sehen wäre, dass sich die Anzahl Kanten in einer Episode ganz anders verhält. Wir sehen in der untenstehenden Grafik zwar Unterschiede, allerdings keine drastischen Gegensätze. Alle Verteilungen sind leicht links schief und die Spannweite ist nie grösser als 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "\n",
    "\n",
    "degree_data = []\n",
    "\n",
    "\n",
    "for episode, graph in enumerate(graph_every_episode, start=1):\n",
    "    # Calculate degrees\n",
    "    degrees = dict(graph.degree())\n",
    "    df_degree = pd.DataFrame({'ID': list(degrees.keys()), 'Degree': list(degrees.values())})\n",
    "    df_degree['Episode'] = episode\n",
    "    degree_data.append(df_degree)\n",
    "\n",
    "# Eigener DF für EDA der gesamten Episoden basteln\n",
    "df_all_degree_data = pd.concat(degree_data)\n",
    "#Farben für Plots festlegen (Farbenblindsicher)\n",
    "colors = sns.color_palette(\"viridis\", n_colors=len(df_all_degree_data['Episode'].unique()))\n",
    "\n",
    "#Facetgrid erstellen für bessere Übersicht\n",
    "g = sns.FacetGrid(df_all_degree_data, col=\"Episode\", col_wrap=4, sharey=False)\n",
    "g.map(plt.hist, \"Degree\", bins=20)\n",
    "\n",
    "g.set_axis_labels(\"Anzahl Knoten\", \"Häufigkeit der Kantenanzahl\")\n",
    "g.set_titles(\"Verteilung der Kantenanzahl pro Knoten - Episode {col_name}\")\n",
    "g.tight_layout()\n",
    "\n",
    "# Für jedes Facet Grid Element Titel und Farbe setzen\n",
    "for ax, episode, color in zip(g.axes.flatten(), df_all_degree_data['Episode'].unique(), colors):\n",
    "    title_text = f'Verteilung der Kantenanzahl\\npro Knoten - Episode {episode}'\n",
    "    ax.set_title('\\n'.join(textwrap.wrap(title_text, 20)), size=10)\n",
    "    for patch in ax.patches:\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Plausibilitätsprüfungen\n",
    "Ob die Daten plausibel sind, kann man auf verschiedene Weisen prüfen. Wir möchten hier vor allem sicherstellen, dass die Anzahl Knoten auch der erwarteten Anzahl Knoten respektive Kanten entspricht, um unerwartetes Verhalten zu verhindern(das haben wir auch oben schon angeschnitten, als wir fehlende Werte gezählt haben). Ebenfalls haben wir überprüft, ob die Range der Werte alle in etwa dem gleichen Bereich liegen, also ob in irgendeiner der Episoden Werte vorkommen, die astronomische (pun intended) Werte annehmen und nicht zu den anderen passen. Ebenso haben wir überprüft, ob wirklich alle Elemente des Graphen miteinander verbunden sind und nochmals ausgegeben, wie viele Kanten jeder Knoten hat um das, falls gewollt, auch nochmals von selbst überprüfen können, dass kein Knoten 0 Verbindungen hat. All unsere Analysen sind unauffällig, das heisst wir gehen davon aus, dass wir es mit plausiblen Daten zu tun haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checken ob die Anzahl erwarteter Knoten auch die tatsächliche Anzahl Knoten ist\n",
    "for episode, (df_nodes, df_edges) in enumerate(lst_every_episode, start=1):\n",
    "    num_nodes_expected = len(df_nodes)  # Adjust this based on your expectations\n",
    "    num_edges_expected = len(df_edges)  # Adjust this based on your expectations\n",
    "\n",
    "    num_nodes_actual = len(graph_every_episode[episode-1].nodes)\n",
    "    num_edges_actual = len(graph_every_episode[episode-1].edges)\n",
    "\n",
    "    print(f\"\\nEpisode {episode} - Expected Node Count: {num_nodes_expected}, Actual Node Count: {num_nodes_actual}\")\n",
    "    print(f\"Episode {episode} - Expected Edge Count: {num_edges_expected}, Actual Edge Count: {num_edges_actual}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Checken ob Gewicht der Kanten und Grösse der Knoten über alle Episoden etwa die gleiche range hat.\n",
    "for episode, (df_nodes, df_edges) in enumerate(lst_every_episode, start=1):\n",
    "\n",
    "    node_size_range = (df_nodes['value'].min(), df_nodes['value'].max())\n",
    "    print(f\"\\nEpisode {episode} - Node Size Range: {node_size_range}\")\n",
    "\n",
    "    edge_weight_range = (df_edges['value'].min(), df_edges['value'].max())\n",
    "    print(f\"Episode {episode} - Edge Weight Range: {edge_weight_range}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for episode, graph in enumerate(graph_every_episode, start=1):\n",
    "    # Prüfen ob der Graph so miteinander verbunden ist, dass alle Elemente zueinander finden können\n",
    "    is_connected = nx.is_connected(graph)\n",
    "    print(f\"\\nEpisode {episode} - Is Connected: {is_connected}\")\n",
    "\n",
    "    #Die Anzahl der Verbindungen pro Knoten wird gleich auch noch mitgegeben\n",
    "    degrees = dict(graph.degree())\n",
    "    print(f\"Episode {episode} - Degree Distribution:\")\n",
    "    print(degrees)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Netzwerkanalyse der ersten Episode\n",
    "\n",
    "Die Funktion ``plot_episode_network`` ermöglicht es uns, ein beliebiges Netzwerk aus unseren Daten zu visualisieren. Diese Funktion erlaubt es uns erste Einblicke in das Netzwerk zu kriegen und dient als erster Anhaltspunkt unserer Netzwerk Visualisierung. Dabei haben wir zwei unterschiedliche Netzwerk Visualisierungsarten. Den Standard bzw. Normalen der mittels ``nx.draw`` oder mittels ``nx.draw_circular``. Dabei sind die beiden wichtigsten Parameter:\n",
    "\n",
    "``graph_list``: Diese Parameter erwartet eine Liste von Graphen, aus der das gewünschte Netzwerk ausgewählt wird.  \n",
    " Die Liste ``graph_every_episode`` enthält alle NetworkX-Graphen, die wir für jede Episode erstellt haben.   \n",
    "``episode_number``: Dieser Parameter gibt die Episode an, die wir visualisieren möchten. \n",
    "`view`: Wechsel zwischen den beiden Visualisierungsarten. `view = standard` für `nx.draw` und `view = circular` für `nx.draw_circular`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episode_network(graph, episode_number=None, node_colors='Lightblue'):\n",
    "    if isinstance(graph, list):\n",
    "        graph_episode = graph[episode_number - 1]\n",
    "    else:\n",
    "        graph_episode = graph\n",
    "    \n",
    "    labels = {node: data['label'] for node, data in graph_episode.nodes(data=True)}\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    nx.draw(\n",
    "        graph_episode,\n",
    "        labels=labels,\n",
    "        with_labels=True,\n",
    "        node_color=node_colors,\n",
    "        node_size=500,\n",
    "        edge_color='black',\n",
    "        font_size=10,\n",
    "        font_weight='bold',\n",
    "        width=0.5)\n",
    "    plt.title(f\"One-Mode Netzwerk von Interaktionen der Star Wars Charakteren der Episode {episode_number}\", fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "plot_episode_network(graph = graph_every_episode, \n",
    "                     episode_number = 1)\n",
    "\n",
    "graph_first_episode = graph_every_episode[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Lassen sich Cluster/Communities erkennen oder gar erstellen?\n",
    "\n",
    "Praktischerweise erlaubt es uns die Library `networkx` die Communities in einem Netzwerk durch die Methode `number_connected_coponents` zu erkennen. Da wir einen ungerichteten Graphen haben ist es nicht verwunderlich, dass wir nur eine Community erkennen können. Aus diesem Grund nutzen wir dann in einem zweiten Ansatz den Edge-Betweenness Clustering Algorithmus auch bekannt aus grivan-newman. Dieser Algorithmus erlaubt es uns die Nodes in Communities zu unterteilen, dies führt der Algorithmus indem er die Edge Betweenness berechnet und die Edge mit der höchsten Betweenness entfernt. Die Funktion hat als Rückgabewert eine Liste von Listen, wobei jede Liste eine Community darstellt.\n",
    "\n",
    "Die Funktion `get_node_groups_grivan_newman` nutzt den Algorithmus von Grivan-Newman um die Nodes in Communities zu unterteilen, der Input ist der Graph selbst. Die Funktion gibt eine Liste von Listen zurück, wobei jede Liste eine Community darstellt.\n",
    "\n",
    "Die Funktion `create_color_map` erstellt eine Liste von Farben, der Input ist der Graph selbst und die Liste von Communities. Die Funktion gibt eine Liste von Farben zurück, wobei jede Farbe einer Community entspricht.\n",
    "\n",
    "Durch die beiden Funktionen `get_node_groups_grivan_newman` und `create_color_map` können wir dann anschliessend das Netzwerk mit der vorherhigen geschriebenen Funktion `plot_epsiode_network` plotten.\n",
    "\n",
    "### 3.1.1 Connected Components\n",
    "\n",
    "Wir überprüfen wie viele «Connected Components» im Netzwerk der ersten Episode vorhanden sind. Da wir allerdings bei der visuellen Inspektion sehen konnten, dass es nur eine Gruppe gibt und unser Netzwerk ungerichtet ist, dürfte es nur ein «Connected Component» geben. Wir überprüfen dies allerdings auch noch hier und geben die Komponenten auch aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anzahl Connected Components:\", nx.number_connected_components(graph_first_episode))\n",
    "\n",
    "print(\"Die Komponenten:\")\n",
    "for x in nx.connected_components(graph_first_episode):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Wie erwartet gibt es nur eine «Connected Component» und alle Charakteren sind Teil davon. Dies sagt uns über den Film und der darin enthaltenen Kommunikation relativ wenig, bestätigt aber unsere obige Annahme über das Netzwerk.\n",
    "\n",
    "### 3.1.2 Erstellung von Edge-Betweenness-Cluster\n",
    "\n",
    "In diesem Schritt führen wir ein Clustering gemäss der Methode von Girvan-Newman aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_groups_grivan_newman(graph, print_node_groups=False):\n",
    "    labels = {node: data['label'] for node, data in graph.nodes(data=True)}\n",
    "    communities = nx.community.girvan_newman(graph)\n",
    "    node_groups = []\n",
    "    for com in next(communities):\n",
    "        node_groups.append(list(com))\n",
    "    if print_node_groups:\n",
    "        for i, group in enumerate(node_groups):\n",
    "            print(f\"Gruppe {i+1}:\")\n",
    "            for node in group:\n",
    "                print(labels[node])\n",
    "            print()\n",
    "    return node_groups\n",
    "\n",
    "node_first_groups = get_node_groups_grivan_newman(graph_first_episode, print_node_groups=False)\n",
    "\n",
    "def create_color_map(graph, node_groups):\n",
    "    color_map = []\n",
    "    for node in graph:\n",
    "        if node in node_groups[0]:\n",
    "            color_map.append('Lightblue')\n",
    "        else:\n",
    "            color_map.append('Limegreen')\n",
    "    \n",
    "    return color_map\n",
    "\n",
    "color_first_map = create_color_map(graph_first_episode, node_first_groups)\n",
    "\n",
    "plot_episode_network(graph = graph_first_episode,\n",
    "                        episode_number = 1,\n",
    "                        node_colors = color_first_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Wie wir sehen, hat der Algorithmus zwei Cluster gebildet. Sie werden in unserer Darstellung oben farblich unterschieden. Es fällt auf, dass einige der Charaktere des kleineren Clusters zur «bösen Seite» gehören. Was allerdings auch wichtig zu sagen und nicht uninteressant ist, ist dass der Imperator (Emperor im Graph) nicht zu diesem «bösen» Cluster gehört, obwohl er ganz klar auch zu dieser Seite gehört. Der Grund dafür erklären wir uns dadurch, dass er viele Kanten zur «guten Seite» hat. Der Imperator ist in der ersten Episode einer der Hauptcharaktere und hat auch viele Interaktionen mit Personen der guten Seite, gerade in Konflikthandlungen. Der Algorithmus hat daher eher Nebencharaktere der «bösen Seite» zusammen gruppiert, diese agieren eher im Hintergrund und haben daher keine oder wenige Verbindungen zu Personen der «guten Seite»."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Gibt es Cliquen?\n",
    "\n",
    "Wir wollen untersuchen, ob es Cliquen innerhalb der ersten Episode vorhanden sind. Dabei verwenden wir die k-Core Clique Methode und untersuchen unterschiedliche k-Werte. Wir können somit Zentrale Charaktere die mit vielen anderen Charakteren im Film interagiert haben erkennen. NetworkX bietet hier die Funktion `nx.k_core` welche als Parameter einen NetworkX Graphen und k-Core Wert annimmt und als Rückgabewert einen Subgraphen mit den k-Core Nodes zurückgibt. Die Funktion `plot_k_core_network` nutzt die Funktion `nx.k_core` um einen Subgraphen zu erstellen und diesen anschliessend zu plotten. Die Funktion `plot_k_core_network` hat als Parameter den Graphen, die entsprechende Episode Folge und den k-Core Wert. Als weiteren Paramaeter haben wir `modus`, welche uns erlaubt zwischen die nicht zu den k-Core Nodes gehörenden Nodes zu verstecken oder nicht. `modus = just_k_core` versteckt die nicht zu den k-Core Nodes gehörenden Nodes und ansonsten versteckt die nicht zu den k-Core Nodes gehörenden Nodes nicht.\n",
    "\n",
    "In unserem Fall testen wir die k-Core Werte 2, 4 und 6, ein Test mit höheren k-Werten ergab keine Clique, weil es keinen Charakter in der ersten Episode gab, der mit mehr als 6 anderen Charakteren interagiert hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_core_network(graph, episode_number, k_core_list, modus=None):\n",
    "    # Define a colormap for assigning colors to k-core clusters\n",
    "    cmap = plt.get_cmap(\"viridis\")  \n",
    "\n",
    "    if modus == \"just_k_core\":\n",
    "        for k_value in k_core_list:\n",
    "            k_core_cluster = nx.k_core(graph, k=k_value)\n",
    "            labels = {node: data['label'] for node, data in k_core_cluster.nodes(data=True)}\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            nx.draw(k_core_cluster, \n",
    "                    labels=labels, \n",
    "                    with_labels=True, \n",
    "                    node_size=500, \n",
    "                    node_color='Lightblue',\n",
    "                    edge_color='black', \n",
    "                    font_size=10, \n",
    "                    font_weight='bold',\n",
    "                    width=0.5)\n",
    "            plt.title(f\"One-Mode Netzwerk von Interaktionen der Star Wars Charakteren der Episode {episode_number} mit k-Core Wert: {k_value}\", fontsize=15)\n",
    "            plt.show()\n",
    "\n",
    "    else:\n",
    "        for k_value in k_core_list:\n",
    "            k_core_cluster = nx.k_core(graph, k=k_value)\n",
    "            labels = {node: data['label'] for node, data in k_core_cluster.nodes(data=True)}\n",
    "            \n",
    "            node_colors = {}\n",
    "            k_core_colors = [cmap(i / k_value) for i in nx.core_number(k_core_cluster).values()]\n",
    "\n",
    "            for node in k_core_cluster.nodes():\n",
    "                node_colors[node] = k_core_colors.pop(0)\n",
    "\n",
    "            for node in graph.nodes():\n",
    "                if node not in node_colors:\n",
    "                    node_colors[node] = 'gray'  \n",
    "            \n",
    "            plt.figure(figsize=(12, 10))\n",
    "            labels = {node: data['label'] for node, data in graph.nodes(data=True)}\n",
    "            node_colors_list = [node_colors[node] for node in graph.nodes()]\n",
    "        \n",
    "            nx.draw(\n",
    "                graph,\n",
    "                labels=labels,\n",
    "                with_labels=True,\n",
    "                node_size=500,\n",
    "                node_color=node_colors_list,\n",
    "                edge_color='black',\n",
    "                font_size=10,\n",
    "                font_weight='bold',\n",
    "                width=0.5)\n",
    "\n",
    "            plt.title(f\"One-Mode Netzwerk von Interaktionen der Star Wars Charakteren der Episode {episode_number} mit k-Core Wert: {k_value}\", fontsize=15)\n",
    "            plt.show()\n",
    "\n",
    "plot_k_core_network(graph=graph_first_episode, episode_number=1, k_core_list=[2, 4, 6])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Wir erkennen in unseren Visualisierungen, dass der Graph weniger Nodes hat und beim k-Core 6 nur noch die Charaktere vorhanden sind, die mit mindestens 6 anderen Charakteren interagiert haben. Dies ist auch die Definition eines k-Cores, unser Code funktioniert also wie gewünscht.\n",
    "\n",
    "Der k-Core Wert 6 ist der höchste Wert, für den eine Clique gefunden werde konnte. Sie ist im letzten Plot ersichtlich. Die Clique besteht zu etwa der Hälfte aus wichtigen Personen der Episode und gesamten Filmreihe: Anakin, Padme, Obi-Wan, Qui-Gon, R2-D2 und C-3PO. Da diese Charakteren viel Kontakt zueinander haben, ist es deshalb zu erwarten, dass sie eine enge Clique miteinander bilden.\n",
    "\n",
    "\n",
    "## 3.3 Wie schnell würde sich die Geburt Yodas im Netzwerk verbreiten?\n",
    "\n",
    "Da es bei Star Wars, wie der Name schon sagt, grösstenteils um Kriege geht, ist es wichtig zu wissen, wie schnell Informationen verbreitet werden. Eine Metrik, die dafür verwendet werden kann, ist der Diameter. Dabei handelt es sich um den längsten kürzesten Pfad im Netzwerk. Es handelt sich also um eine Entfernung und gibt an, wieviele Kanten maximal überwunden werden müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anzahl Kanten:\", nx.diameter(graph_first_episode))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der ersten Episode beträgt der Diameter 4. Die Nachricht von zum Beispiel einer Geburt Yodas oder einem neuen Krieg, wird also nach spätestens vier Kanten alle Charaktere erreichen. Mangels Vergleichswerten kann die Anzahl nur schwer interpretiert werden, uns kommt sie aber relativ klein und effizient vor.\n",
    "\n",
    "Eine Überprüfung der Kennzahl Diameter ist im originalen Netzwerk nicht gut möglich, da es recht unübersichtlich ist. Da wir gerade k-Core Clique erstellt haben, überprüfen wir die Metrik aber mit der Clique mit dem höchsten möglichen k-Wert (k=6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anzahl Kanten:\", nx.diameter(nx.k_core(graph_first_episode, k=6)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Diameter beträgt 2. In dem Netzwerk konnten wir keine grösseren kürzesten Pfad feststellen. Auftreten tut der Wert relativ häufig, zum Beispiel zwischen den Charakteren Jabba-Kitster, Sebula-Anakin oder Qui-Gon-Watto.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Netzwerkanalyse der Entwicklungen im Verlauf der Episoden\n",
    "\n",
    "\n",
    "Die Funktion `calculate_actor_centrality` berechnet verschiedene zentrale Kennzahlen (Centrality) für alle Charaketere in einem Graphen. \n",
    "Diese Kennzahlen helfen dabei, die Bedeutung oder Zentralität jeden Charaketer im Netzwerk zu quantifizieren.\n",
    "\n",
    "Die Funktion nimmt als Parameter eine Liste von Graphen entgegen und gibt ein Dataframe zurück, welches die Charaktere und die entsprechenden Centrality Werte enthält. Dies ermöglicht es uns, die Entwicklung der Zentralität der Charaktere im Laufe der Episoden zu untersuchen und einfacher zu visualisieren sowie zu analysieren. \n",
    "\n",
    "Die Funktion ``plot_actor_metrics`` dient dazu, die Entwicklung der Zentralität von Charakteren im Verlauf der Episoden grafisch darzustellen. Als Parameter erwartet die Funktion das Dataframe, das aus der Funktion ``calculate_actor_centrality`` stammt, den Namen des Charakters, dessen Zentralitätsentwicklung wir visualisieren möchten, sowie den gewählten Visualisierungstyp.\n",
    "\n",
    "Es stehen zwei Visualisierungstypen zur Auswahl: ein Liniendiagramm und ein Balkendiagramm. Die Interpretation der Visualisierung mittels eines Liniendiagramms ist oft einfacher, da die Entwicklung der Zentralität des Charakters im Laufe der Episoden besser erkennbar ist im Vergleich zum Balkendiagramm. Allerdings hat das Liniendiagramm den Nachteil, dass Charaktere, die am Anfang und am Ende einer Episode auftauchen, eine durchgehende Linie im Diagramm haben, was zu einer möglichen fehlerhaften Interpretation führen könnte. Einen Kompromiss gehen wir mit einem Scatterdiagramm ein indem wir die Punkte mit einer dashed Linie verbinden. Weiter um die INterpretierbarkeits fehler zu minimieren haben wir bewusst die Visualisierung interaktiv mittels Plotly umgesetzt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_actor_centrality(graph_every_episode):\n",
    "    dfs = []\n",
    "    for episode in graph_every_episode:\n",
    "        # Calculate degree centrality\n",
    "        degree_dict = nx.degree(episode)\n",
    "        name_degree_list = [(episode.nodes[node]['label'], degree) for node, degree in degree_dict]\n",
    "        df = pd.DataFrame(name_degree_list, columns=['Actor', 'Degree Centrality'])\n",
    "        # calculate normalized degree centrality\n",
    "        normalized_degree_dict = nx.degree_centrality(episode)\n",
    "        name_normalized_degree_list = [(episode.nodes[node]['label'], normalized_degree) for node, normalized_degree in normalized_degree_dict.items()]\n",
    "        df['Normalized Degree Centrality'] = [normalized_degree for actor, normalized_degree in name_normalized_degree_list]\n",
    "        # Calculate closeness centrality\n",
    "        closeness_dict = nx.closeness_centrality(episode)\n",
    "        name_closeness_list = [(episode.nodes[node]['label'], closeness) for node, closeness in closeness_dict.items()]\n",
    "        df['Closeness Centrality'] = [closeness for actor, closeness in name_closeness_list]\n",
    "        # Calculate betweenness centrality\n",
    "        betweenness_dict = nx.betweenness_centrality(episode)\n",
    "        name_betweenness_list = [(episode.nodes[node]['label'], betweenness) for node, betweenness in betweenness_dict.items()]\n",
    "        df['Betweenness Centrality'] = [betweenness for actor, betweenness in name_betweenness_list]\n",
    "        # Calculate Clustering Coefficient\n",
    "        clustering_dict = nx.clustering(episode)\n",
    "        name_clustering_list = [(episode.nodes[node]['label'], clustering) for node, clustering in clustering_dict.items()]\n",
    "        df['Clustering Coefficient'] = [clustering for actor, clustering in name_clustering_list]\n",
    "        # Create a column with the episode number\n",
    "        df['Episode'] = graph_every_episode.index(episode) + 1\n",
    "        dfs.append(df)\n",
    "    df_degree_centrality = pd.concat(dfs, ignore_index=True)\n",
    "    return df_degree_centrality\n",
    "\n",
    "actor_centrality_all = calculate_actor_centrality(graph_every_episode)\n",
    "display(actor_centrality_all)\n",
    "\n",
    "def plot_actor_metrics(dataframe, actors, metric_vis, plot_type=\"bar\"):\n",
    "    centrality_pick = dataframe[dataframe['Actor'].isin(actors)]\n",
    "    if plot_type == \"bar\":\n",
    "        fig = px.bar(centrality_pick, x=\"Episode\", y=metric_vis, color=\"Actor\", barmode=\"group\")\n",
    "    elif plot_type == \"line\":\n",
    "        fig = px.line(centrality_pick, x=\"Episode\", y=metric_vis, color=\"Actor\")\n",
    "    elif plot_type == \"scatter\":\n",
    "        fig = px.scatter(centrality_pick, x=\"Episode\", y=metric_vis, color=\"Actor\")\n",
    "        fig.update_traces(mode='lines+markers', marker=dict(size=10), line=dict(width=4, dash='dot'))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid plot_type. Use 'bar' or 'line' or 'scatter'.\")\n",
    "    \n",
    "    actors = centrality_pick['Actor'].unique()\n",
    "    title = f'{metric_vis} for {\", \".join(actors)}'\n",
    "    fig.update_layout(title_text=title)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_of(datapath, interested_col=\"species\", species=\"NaN\", display_df = False):\n",
    "    characters_data = pd.read_csv(datapath)\n",
    "    if display_df:\n",
    "        display(characters_data)\n",
    "    characters_droid = characters_data[characters_data[interested_col] == species]\n",
    "    characters_droid = characters_droid['name'].tolist()\n",
    "    return characters_droid\n",
    "\n",
    "characters_droid = get_names_of(datapath='data/Star Wars/characters.csv', interested_col=\"species\", species=\"Droid\", display_df = True)\n",
    "print(\"Stawrwars Droiden Charaktere:\", characters_droid)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie es scheint, kommen nur R2-D2 und C3PO in allen Episoden vor. Wir werden deshalb die Vergleiche anhand dieser beider Charakteren durchführen.\n",
    "\n",
    "## 4.1 Wie entwickeln sich die Wichtigkeiten einzelner Charakteren/Gruppen im Verlauf der Filme?\n",
    "\n",
    "### 4.1.1 Degree-Zentralität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actor_metrics(actor_centrality_all, actors = characters_droid, metric_vis='Normalized Degree Centrality', plot_type='scatter')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Closeness Zentralität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actor_metrics(actor_centrality_all, actors = characters_droid, metric_vis='Closeness Centrality', plot_type='scatter')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Betweenness Zentralität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actor_metrics(actor_centrality_all, actors = characters_droid, metric_vis='Betweenness Centrality', plot_type='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actor_metrics(actor_centrality_all, actors = characters_droid + [\"PADME\"], metric_vis='Betweenness Centrality', plot_type='scatter')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerne hätten wir auch die Prestige berechnet, da es sich bei unseren Netzwerken um ungerichtete handelt, ist dies leider nicht möglich.\n",
    "\n",
    "# 5 Entwicklung der Film Netzwerke\n",
    "## 5.1 Wie entwickeln sich die Zentralisierung der Netzwerke?\n",
    "\n",
    "Metriken berechnen zu jedem Film und Visualisieren\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Alle Netzwerke Visualisieren\n",
    "\n",
    "Bevor wir mit den Berechnungen der Metriken anfangen, visualisieren wir alle Netzwerke, um uns einen Überblick zu verschaffen.\n",
    "\n",
    "Die Funktion `plot_all_networks` nimmt als Parameter eine Liste von Graphen entgegen und visualisiert diese in einem Grid. Wir können somit mit dieser Funktion beliebige Episoden nebeneinander visualisieren. \n",
    "\n",
    "Durch die Visualisierung erkennen wir, dass die Anzahl an Charakteren unterschiedlich schwankt. Episode 5 weisst deutlich weniger Anzahl an Charakteren auf als bei Episode 1. Diese Erkentniss ist auch ebenfalls aus dem Kapitel 2, der explorativen Datenanalyse zu sehen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_networks(episode_numbers, graph_data_list, figsize=(17, 7)):\n",
    "    num_episodes = len(episode_numbers)\n",
    "    num_columns = 2  \n",
    "\n",
    "    num_rows = (num_episodes + num_columns - 1) // num_columns\n",
    "    fig, axes = plt.subplots(num_rows, num_columns, figsize=(figsize[0] * num_columns, figsize[1] * num_rows))\n",
    "\n",
    "    for i, episode_number in enumerate(episode_numbers):\n",
    "        if episode_number < 1 or episode_number > len(graph_data_list):\n",
    "            print(f\"Episode {episode_number} existiert nicht.\")\n",
    "            continue\n",
    "\n",
    "        selected_graph = graph_data_list[episode_number - 1]\n",
    "\n",
    "        labels = {node: data['label'] for node, data in selected_graph.nodes(data=True)}\n",
    "\n",
    "        row = i // num_columns\n",
    "        col = i % num_columns\n",
    "\n",
    "        ax = axes[row, col] if num_rows > 1 else axes[col]\n",
    "        plt.sca(ax)\n",
    "\n",
    "        plt.title(f\"One-Mode Netzwerk von Interaktionen der Star Wars Charakteren der Episode {episode_number}\", fontsize=figsize[0] * 1.5)\n",
    "\n",
    "        nx.draw(selected_graph,\n",
    "                labels=labels,\n",
    "                with_labels=True,\n",
    "                node_size=500,\n",
    "                edge_color='black',\n",
    "                font_size=10,\n",
    "                font_weight='bold',\n",
    "                width=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_all_networks(episode_numbers = [1, 2, 3, 4, 5, 6, 7],\n",
    "                 graph_data_list=graph_every_episode, figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Verteilung der Actor Zenralitäten\n",
    "\n",
    "Hier in diesem Abschnitt visualisieren wir die Verteilung der Aktor Zentralitäten. Die Funktion `plot_centrality_distribution` erlaubt es uns eine bestimmte Centrality Metrik zu visualisieren. Weiter wurde die Funktion so ausgebaut, dass man auch die gewünschte Episode auswählen kann. Die Funktion nimmt als Parameter das Dataframe, welches aus der Funktion `calculate_actor_centrality` stammt, die gewünschte Centrality Metrik, die gewünschte Episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_centrality_distribution(df, metric_vis=\"Degree Centrality\", alpha=0.5, episode_to_show=None):\n",
    "    \n",
    "    # filter for defined episode\n",
    "    if episode_to_show is not None:\n",
    "        df = df[df['Episode'].isin(episode_to_show)]\n",
    "    \n",
    "    fig = px.histogram(df, x=f\"{metric_vis}\", color=\"Episode\", marginal=\"box\", nbins=10, opacity=alpha)\n",
    "    fig.update_layout(title_text=f\"{metric_vis} Actor Verteilung\", barmode=\"overlay\")\n",
    "    fig.show()\n",
    "\n",
    "plot_centrality_distribution(actor_centrality_all, metric_vis=\"Degree Centrality\", alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Funktion für Berechnung der Netzwerk Metriken\n",
    "\n",
    "Für die Berechnung der Metriken haben wir eine Classe geschrieben, welche uns erlaubt die Netzwerk Metriken für jeden Graphen zu berechnen. Die Classe `NetworkCentrality` wird dabei mit dem Dataframe `actor_centrality_all` aus Kapitel 4 Instanziert. In der Classe selbst gibt es eine Methode namens `calculate_network_centrality`, welche für jede Starwars Episoden Netzwerk die folgenden drei Netzwerkmetriekn, welche wir aus dem San Modul gelernt haben berechnet: Degree Network Centrality, Closeness Network Centrality und Betweenness Network Centraltity. Wir berechnen alle Netzwerk Metriken, damit wir diese anschliessend im späteren Kapitel visualiseren können, wie sich die Metriken im Verlauf der Episoden entwickeln haben.  \n",
    "   \n",
    "Als Reminder:  \n",
    "\"Die Netzwerk Zentralisierung kombiniert alle Aktor Zentralitäts-Werte zu einem einzelnen Wert, der\n",
    "aussagt, wie gross die Ungleichheit der Aktor Zentralitäts-Werte innerhalb des Netzwerkes sind\" (Soziale Netzwerkanalyse ©Michael Henninger Juli 2023, Version 3.7-beta).\n",
    "\n",
    "Also anderst gesagt:   \n",
    "Die Netzwerkzentralisierung ist ein Konzept in der Netzwerktheorie, das die Verteilung der Zentralitäts-Werte von Aktoren in einem Netzwerk bewertet. Diese Zentralitäts-Werte messen die Bedeutung oder Zentralität eines Aktors im Netzwerk, und es gibt verschiedene Messmethoden wie Degree Centrality, Closeness Centrality und Betweenness-Centrality. Die Netzwerkzentralisierung fasst diese Werte zusammen, um die Ungleichheit der Zentralitäts-Werte im Netzwerk zu quantifizieren. Hohe Netzwerkzentralisierung bedeutet, dass wenige Aktoren eine hohe Zentralität haben und die meisten eine niedrigere Zentralität aufweisen. Niedrige Netzwerkzentralisierung zeigt an, dass die Zentralitäts-Werte gleichmässiger verteilt sind, was auf eine gleichmässigere Verteilung von Macht oder Einfluss im Netzwerk hinweisen kann.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkCentrality():\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.actors = self.dataframe['Actor'].unique()\n",
    "        self.episodes = self.dataframe['Episode'].unique()\n",
    "\n",
    "    # calculate n for each episode\n",
    "    def _n_actor_for_episode(self, episode):\n",
    "        return len(self.dataframe[self.dataframe['Episode'] == episode]['Actor'].unique())\n",
    "    \n",
    "    # get maximum value of centrality for each episode\n",
    "    def _max_centrality_for_episode(self, episode, centrality):\n",
    "        return self.dataframe[self.dataframe['Episode'] == episode][centrality].max()\n",
    "    \n",
    "    # get degree centrality for each episode\n",
    "    def _degree_centrality(self, episode):\n",
    "        return self.dataframe[self.dataframe['Episode'] == episode]['Degree Centrality']\n",
    "    \n",
    "    # get closeness centrality for each episode\n",
    "    def _closeness_centrality(self, episode):\n",
    "        return self.dataframe[self.dataframe['Episode'] == episode]['Closeness Centrality']\n",
    "    \n",
    "    # get betweenness centrality for each episode\n",
    "    def _betweenness_centrality(self, episode):\n",
    "        return self.dataframe[self.dataframe['Episode'] == episode]['Betweenness Centrality']\n",
    "    \n",
    "    # calculate Degree Network Centrality \n",
    "    def degree_network_centrality(self, episode):\n",
    "        \"\"\"\n",
    "        implementation based on san script michael henninger\n",
    "        \"\"\"\n",
    "        c_max = self._max_centrality_for_episode(episode, 'Degree Centrality')\n",
    "        c = self._degree_centrality(episode)\n",
    "        n = self._n_actor_for_episode(episode)\n",
    "        return (np.sum(c_max - c)) / ((n - 1) * (n - 2))\n",
    "    \n",
    "    # calculate Closeness Network Centrality\n",
    "    def closeness_network_centrality(self, episode):\n",
    "        \"\"\"\n",
    "        implementation based on san script michael henninger,\n",
    "        closnes centrality must be normalized!\n",
    "        \"\"\"\n",
    "        c_max = self._max_centrality_for_episode(episode, 'Closeness Centrality')\n",
    "        c = self._closeness_centrality(episode)\n",
    "        n = self._n_actor_for_episode(episode)\n",
    "        return (np.sum(c_max - c)) / ((n - 2) / 2)\n",
    "    \n",
    "    # calculate Betweenness Network Centrality\n",
    "    def betweenness_network_centrality(self, episode):\n",
    "        \"\"\"\n",
    "        implementation based on san script michael henninger\n",
    "        Betweenness centrality must be normalized!\n",
    "        \"\"\"\n",
    "        c_max = self._max_centrality_for_episode(episode, 'Betweenness Centrality')\n",
    "        c = self._betweenness_centrality(episode)\n",
    "        n = self._n_actor_for_episode(episode)\n",
    "        return (np.sum(c_max - c)) / (n - 1)\n",
    "    \n",
    "    def calculate_network_centrality(self):\n",
    "        \"\"\"\n",
    "        implementation based on san script michael henninger\n",
    "        \"\"\"\n",
    "        dfs = []\n",
    "        for episode in self.episodes:\n",
    "            df_episode = pd.DataFrame({'Episode': [episode],\n",
    "                                    'Degree Network Centrality': [self.degree_network_centrality(episode)],\n",
    "                                    'Closeness Network Centrality': [self.closeness_network_centrality(episode)],\n",
    "                                    'Betweenness Network Centrality': [self.betweenness_network_centrality(episode)]})\n",
    "            dfs.append(df_episode)\n",
    "        \n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        return df\n",
    "    \n",
    "    # visualize network centrality\n",
    "    def plot_network_centrality(self, plot_type=\"bar\", centrality_to_show=[\"Degree Network Centrality\", \"Closeness Network Centrality\", \"Betweenness Network Centrality\"]):\n",
    "        df = self.calculate_network_centrality()\n",
    "        centrality_to_show = centrality_to_show\n",
    "        if plot_type == \"bar\":\n",
    "            fig = px.bar(df, x=\"Episode\", y=centrality_to_show, barmode=\"group\", title=f\"Network Centrality for {', '.join(centrality_to_show)}\")\n",
    "        elif plot_type == \"line\":\n",
    "            fig = px.line(df, x=\"Episode\", y=centrality_to_show)\n",
    "        elif plot_type == \"scatter\":\n",
    "            fig = px.scatter(df, x=\"Episode\", y=centrality_to_show)\n",
    "            fig = fig.update_traces(mode='lines+markers')\n",
    "        else:\n",
    "            raise ValueError(\"Invalid plot_type. Use 'bar' or 'line' or 'scatter'.\")\n",
    "        \n",
    "        title = f'Network Centrality for: {\", \".join(centrality_to_show)}'\n",
    "        fig.update_layout(title_text=title)\n",
    "        fig.show()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1 Initialisierung der Klasse\n",
    "\n",
    "Hier in diesem Abschnitt instanzieren wir die Klasse mit dem Dataframe `actor_centrality_all` aus Kapitel 4 und wenden die Methode `calculate_network_centrality` an. Die Methode berechnet die Netzwerk Metriken für jede Episode und speichert diese in einem Dataframe ab. Dis tun wir wir, weil wir mit dem Dataframe später einfacher die Metriken analysieren und visualisieren können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initliaze NetworkCentrality class\n",
    "network_centrality = NetworkCentrality(actor_centrality_all)\n",
    "\n",
    "# calculate network centrality for each episode\n",
    "network_centrality_df = network_centrality.calculate_network_centrality()\n",
    "display(network_centrality_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Degree Zentralisierung \n",
    "\n",
    "Aus dem San Skirpt kennen wir die Formel für die Berechnung der Degree Zentralisierung auf ungerichtet Graphen.   \n",
    "Die Formel lautet wie folgt:\n",
    "\n",
    "\\begin{align*}\n",
    "Ungerichteter Graph  => CD = \\frac{\\sum_{i=1}^{n}  C'D(p^*) - C'D(p_i)}{(n-1)(n-2)} \n",
    "\\end{align*}\n",
    "\n",
    "Bei einem Gerichteten Graphen fällt im Nenner der Term $(n-2)$ weg.\n",
    "\n",
    "Hier in diesem Degree Zentralisierung von unserem Starwars Episoden Netzwerk erkennen wir, dass sich der Wertebreich sich zwischen 0.38 und 0.56 bewegt. Wir erkennen auch, dass in der Episode 5 der tiefste Degree Zentralisierungs und in der Episode 6 der höchste Wert vorhanden ist. Wir können somit interpretieren, dass in der Episode 5 die Actor Degree Zentrailität Charaktere gleichmässiger bzw besser verteilt sind und in der Episode 6 die Actor Degree Zentralität ungleichmässiger verteilt sind. Anderst gesagt bedeutet dass, dass es einen Zentraleren wichtigeren Charakter in der 6. Folge exisitert. Generell können wir jedoch sagen, dass sich die Netzwerk Zentralisierung im Verlaufe der Episoden sich nur minimal voneinander unterscheiden. Wir können diese Aussage untermauern, indem wir die Actor Zentralitätswert für die Episode 5 und 6 vergleichen. Wir erkennen im Verteilungsplot, dass in der Episode 6 es einen Charakter gibt, der eine sehr hohe Normalizierte Degree Centrality Wert aufweist und während in der Episode 5, die Charaktere gleichmässiger verteilt sind. Nun können wir mittels einem einfachen Filter Statement herausfinden, welcher Charakter nun zuständig ist für den hohen Degree Network Centrality Wert, indem wir den höchsten Wert aus dem Dataframe `actor_centrality_all` herausfiltern. Es ist der Charakter `LUKE` der im Film 6 eine der Zentralsten Rolle spielt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize Degree Network Centrality for each episode\n",
    "network_centrality.plot_network_centrality(plot_type=\"line\", centrality_to_show=[\"Degree Network Centrality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot centrality distribution for episode 5 and 6\n",
    "plot_centrality_distribution(actor_centrality_all, metric_vis=\"Normalized Degree Centrality\", episode_to_show=[5, 6], alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for highest normalized degree centrality for episode 6\n",
    "actor_centrality_all[actor_centrality_all['Episode'] == 6].sort_values(by='Normalized Degree Centrality', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Closeness Zentralisierung\n",
    "\n",
    "Aus dem San Skirpt kennen wir die Formel für die Berechnung der Closeness Zentralisierung auf ungerichtet Graphen.\n",
    "\n",
    "\\begin{align*}\n",
    "CC &= \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\frac{C'(p^*) - C'(p_i)}{n-2} \\right] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Hier in dieser Visualisierung sehen wir die Closeness Zentralisierung von unserem Starwars Episoden Netzwerk. Wir sehen, dass sich der Wert in diesen ersten drei Episoden kontuierlich sinkt, dass bedeutet, dass die Closeness Werte für die Charaktere in diesen Episoden sich immer mehr annähern. In der Episode 4 steigt der Wert wieder an und sinkt aber jedoch in der darauffolgenden 5 Episode. In der 6 Episoden steigt diese dann zum maximalen Wert 0.65 an, dies bedeutet, dass in dieser Episode 6, die Aktor Closeness Centrality Werte sich am meisten voneinander unterscheiden. In der letzten Epsiode 7, sinkt der Wert auf den tiefsten Closeeness Zentralisierung Wert 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize Closeness Network Centrality for each episode\n",
    "network_centrality.plot_network_centrality(plot_type=\"line\", centrality_to_show=[\"Closeness Network Centrality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot centrality distribution for episode 6 and 7\n",
    "plot_centrality_distribution(actor_centrality_all, metric_vis=\"Closeness Centrality\", episode_to_show=[6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for highest closeness centrality for episode 6\n",
    "actor_centrality_all[actor_centrality_all['Episode'] == 6].sort_values(by='Closeness Centrality', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Betweenness Zentralisierung \n",
    "\n",
    "Aus dem San Skirpt kennen wir die Formel für die Berechnung der Betweenness Zentralisierung.\n",
    "\n",
    "\\begin{align*}\n",
    "C\\small{B} = \\frac{\\sum_{i=1}^{n} C'\\small{B}(p^*) - C'\\small{B}(p_i)}{(n-1)}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Hier ist der Verlauf verglichen zu den vorherhigen Netzwerk Zentralitäts Metriken ab der 3. Episode bis zur 6. Episode interessant. Vom tiefesten Beteeweenness Zentralisierung Wert 0.15 in der 3. Episode steigt diese kontiuerlich weiter an bis zur 6. Episode und erreicht seinen höchsten Wert mit einem Peak von 0.57. Ein tiefer Wert bedeutet, dass die Actor Betweenness Centrality Werte sich ähnlicher sind und ein hoher Wert bedeutet, dass die Actor Betweenness Centrality Werte sich stark voneinander unterscheiden. Wir könnten somit interpretieren, dass in der 6. Episode die Actor Betweenness Centrality Werte sich am meisten voneinander unterscheiden und wenige wichtigere Charaktere Schlüsselpositionen im Netzwerk einnehmen, während dies in der 3. Episode nicht der Fall war."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize Betweenness Network Centrality for each episode\n",
    "network_centrality.plot_network_centrality(plot_type=\"line\", centrality_to_show=[\"Betweenness Network Centrality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot centrality distribution for episode 6 and 7\n",
    "plot_centrality_distribution(actor_centrality_all, metric_vis=\"Betweenness Centrality\", episode_to_show=[3, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for highest betweenness centrality for episode 6\n",
    "actor_centrality_all[actor_centrality_all['Episode'] == 6].sort_values(by='Betweenness Centrality', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Alle Netzwerk Zentralisierungs Metriken für alle Episoden\n",
    "\n",
    "Vollständigkeitshalber visualisieren wir alle drei Netzwerk Metriken für alle Episoden, farblich voneinander getrennt. Wir erkennen, dass sich die Netzwerk Metriken von Degree und Closeness einen ähnlichen Verlauf haben, während die Betweenness Zentralisierung sich von den beiden anderen Metriken unterscheidet. Jedoch sehr interessant ist die 6. Episode, denn in dieser Episode haben alle Netzwerke den maximalen höchsten Netzwerk Zentralitäts Wert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize all network centrality metrics for each episode\n",
    "network_centrality.plot_network_centrality(plot_type=\"line\", centrality_to_show=[\"Degree Network Centrality\", \"Closeness Network Centrality\", \"Betweenness Network Centrality\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Netzwerk Metriken\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Graph Density\n",
    "\n",
    "Dokuementation von Graph Density in NetworkX: https://networkx.org/documentation/stable/reference/generated/networkx.classes.function.density.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_graph_density(graph_every_episode):\n",
    "    dfs = []\n",
    "    for G_episode in graph_every_episode:\n",
    "        density = nx.density(G_episode)\n",
    "        dfs.append(density)\n",
    "    df_density = pd.DataFrame(dfs, columns=['Density'])\n",
    "    df_density = df_density.reset_index(drop=False).rename(columns={\"index\": \"Episode\"})\n",
    "    df_density['Episode'] = df_density['Episode'] + 1\n",
    "    return df_density\n",
    "\n",
    "df_density = calculate_graph_density(graph_every_episode)\n",
    "display(df_density)\n",
    "\n",
    "fig = px.line(df_density, x=\"Episode\", y=\"Density\")\n",
    "fig.update_layout(title_text=\"Dichte der Star Wars Episode Netzwerke\")\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Cluster Coefficient einzelner Nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"Cluster Coefficient for each node:\", actor_centrality_all[[\"Actor\", \"Episode\", \"Clustering Coefficient\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actor_metrics(actor_centrality_all, actors = ['R2-D2', 'C-3PO'], metric_vis='Clustering Coefficient', plot_type='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Cluster Coefficient der Netzwerke "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_clustering_coefficient(graph_every_episode):\n",
    "    dfs = []\n",
    "    for G_episode in graph_every_episode:\n",
    "        avg_clustering_coefficient = nx.average_clustering(G_episode)\n",
    "        dfs.append(avg_clustering_coefficient)\n",
    "    df_avg_clustering_coefficient = pd.DataFrame(dfs, columns=['Clustering Coefficient'])\n",
    "    df_avg_clustering_coefficient = df_avg_clustering_coefficient.reset_index(drop=False).rename(columns={\"index\": \"Episode\"})\n",
    "    df_avg_clustering_coefficient['Episode'] = df_avg_clustering_coefficient['Episode'] + 1\n",
    "    return df_avg_clustering_coefficient\n",
    "\n",
    "df_avg_clustering_coefficient = average_clustering_coefficient(graph_every_episode)\n",
    "display(\"Average Cluster Coefficient for each Episode:\", df_avg_clustering_coefficient)\n",
    "\n",
    "fig = px.line(df_avg_clustering_coefficient, x=\"Episode\", y=\"Clustering Coefficient\")\n",
    "fig.update_layout(title_text=\"Durchschnittlicher Clustering Coefficient der Star Wars Episode Netzwerke\")\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Link-Prediction\n",
    "\n",
    "Bei der Link Prediction wird versucht vorherzusagen, welche Charakteren eine direkte Verbindung (also Kante) aufbauen sollten.\n",
    "\n",
    "Für die Link Predictions verwenden wir die drei Methoden Jaccard Coefficient, Preferential Attachment und Resource Allocation. Wir haben sie ausgewählt, weil sie einerseits unterschiedliche Berechnungsmethoden haben und andererseits, weil sie komplexer als z.B. die Common Neighbours Methode sind.\n",
    "\n",
    "Aufgrund der unterschiedlichen Berechnungsmethoden wird es interessant sein, zu sehen, wie unterschiedlich die Link Predictions sind. Zusätzlich werden wir einzelne Predictions genauer anschauen und so weit möglich die Berechnung von Networkx überprüfen.\n",
    "\n",
    "Zum Schluss werden wir auch eine Methode für eine mögliche Anwendung bestimmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_prediction_methods = [\"jaccard_coefficient\", \"preferential_attachment\", \"resource_allocation_index\"]\n",
    "lst_predictions = []\n",
    "\n",
    "for method in lst_prediction_methods:\n",
    "    method = getattr(nx, method)\n",
    "    predictions = method(graph_first_episode)\n",
    "    lst_predictions.append(list(predictions))\n",
    "\n",
    "labels = {node: data['label'] for node, data in graph_first_episode.nodes(data=True)}\n",
    "\n",
    "for method_name, method_predictions in zip(lst_prediction_methods, lst_predictions):\n",
    "    sorted_link_pred = sorted(method_predictions, key=lambda x: x[2], reverse=True)\n",
    "    top_5_recommendations = sorted_link_pred[:5]\n",
    "\n",
    "    top_5_recommendations_names = [(labels[u], labels[v], p) for u, v, p in top_5_recommendations]\n",
    "\n",
    "    print(f\"Top 5 Empfehlungen mit {method_name}:\")\n",
    "    for u, v, p in top_5_recommendations_names:\n",
    "        print(u, \"und\", v, \"Wert:\", round(p, 3))\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für jede der drei Methoden wurden die Top fünf Empfehlungen angezeigt. Wir stellen fest, dass die Methoden recht unterschiedliche Resultate generiert haben. Zwar kommen häufig ähnliche Charakteren vor, doch meistens in anderen Kombinationen. Die Wahl der Methode hat also einen wichtigen Einfluss auf die erstellten Predictions.\n",
    "\n",
    "Folgend werden wir die Top-Empfehlungen der einzelnen Methoden genauer anschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_characters(lst_prediction_methods, lst_predictions, index, n_characters, top_n_pred):\n",
    "    method_name = lst_prediction_methods[index]\n",
    "    method_predictions = lst_predictions[index]\n",
    "\n",
    "    sorted_link_pred = sorted(method_predictions, key=lambda x: x[2], reverse=True)\n",
    "    top_n_recommendations = sorted_link_pred[:top_n_pred]\n",
    "\n",
    "    char_counter = Counter()\n",
    "    for u, v, _ in top_n_recommendations:\n",
    "        char_counter[labels[u]] += 1\n",
    "        char_counter[labels[v]] += 1\n",
    "\n",
    "    top_n_chars = char_counter.most_common(n_characters)\n",
    "\n",
    "    print(f\"Top {n_characters} am häufigsten empfohlene Charaktere in den Top {top_n_pred} Empfehlungen mit {method_name}:\")\n",
    "    for char, count in top_n_chars:\n",
    "        print(f\"{char}: {count} Mal\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Jaccard Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_characters(lst_prediction_methods, lst_predictions, 0, 7, 30)\n",
    "\n",
    "print()\n",
    "print(\"Untersuchung einzelner Top Predictions:\")\n",
    "\n",
    "print()\n",
    "print(\"Anzahl Nachbarn von Sio Bibble:\", nx.degree(graph_first_episode, 11), \"und von Boss Nass:\", nx.degree(graph_first_episode, 14))\n",
    "print(\"Nachbarn von Sio Bibble:\", sorted(graph_first_episode.neighbors(11)))\n",
    "print(\"Nachbarn von Boss Nass:\", sorted(graph_first_episode.neighbors(14)))\n",
    "print(\"Anzahl gemeinsamer Nachbarn:\", len(sorted(nx.common_neighbors(graph_first_episode, 11, 14))))\n",
    "\n",
    "print()\n",
    "print(\"Anzahl Nachbarn von R2-D2:\", nx.degree(graph_first_episode, 0), \"und von Jira:\", nx.degree(graph_first_episode, 20))\n",
    "print(\"Nachbarn von R2-D2:\", sorted(graph_first_episode.neighbors(0)))\n",
    "print(\"Nachbarn von Jira:\", sorted(graph_first_episode.neighbors(20)))\n",
    "print(\"Anzahl gemeinsamer Nachbarn:\", len(sorted(nx.common_neighbors(graph_first_episode, 0, 20))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boss Nass und Jira werden mit je sieben Nennungen am häufigsten in den Top 30 Empfehlungen genannt. Wie wir bereits früher festgestellt haben, sind das Charakteren mit eher weniger Nachbarn. Entsprechend ist es vermutlich wahrscheinlicher, dass sie mit anderen Charakteren starke Übereinstimmungen der Nachbarn haben, was einen höheren Koeffizienten ergibt.\n",
    "\n",
    "Bei der Analyse der ersten Prediction (Sio Bibble und Boss Nass) stellen wir fest, dass sie sieben und sechs Nachbarn haben, fünf davon sind gemeinsam. Dividieren wir diesen Wert durch acht, der Anzahl unterschiedlicher Nachbarn des Paars, erhalten wir den Koeffizient 0.625. Diesen Wert hat auch Networkx berechnet.\n",
    "\n",
    "Bei der fünften Empfehlung (R2-D2 und Jira) gibt es nur drei gemeinsame Nachbarn, die sechs unterschiedlicher Nachbarn gegenüber stehen. Entsprechend ist der Koeffizient mit 0.5 tiefer.\n",
    "\n",
    "## 6.2 Preferential Attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_characters(lst_prediction_methods, lst_predictions, 1, 7, 30)\n",
    "\n",
    "print()\n",
    "print(\"Untersuchung einzelner Top Predictions:\")\n",
    "print()\n",
    "print(\"Anzahl Nachbarn von Emperor:\", nx.degree(graph_first_episode, 9), \"und von Anakin:\", nx.degree(graph_first_episode, 18))\n",
    "print(\"Multiplikation der Anzahl Nachbarn:\", nx.degree(graph_first_episode, 9) * nx.degree(graph_first_episode, 18))\n",
    "print()\n",
    "print(\"Anzahl Nachbarn von Obi-Wan:\", nx.degree(graph_first_episode, 5), \"und von Emperor:\", nx.degree(graph_first_episode, 9))\n",
    "print(\"Multiplikation der Anzahl Nachbarn:\", nx.degree(graph_first_episode, 5) * nx.degree(graph_first_episode, 9))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anakin und Jar Jar werden mit je acht Nennungen am häufigsten in den Top 30 Empfehlungen genannt. Das sind beides Charakteren mit einer relativ hohen Anzahl Nachbarn. Aufgrund der Berechnungsweise der Methode ist es zu erwarten, dass solche Charakteren häufig empfohlen werden. Denn mögliche Predictions mit ihnen bekommen schneller einen hohen Koeffizienten.\n",
    "\n",
    "Die Analyse der Top Empfehlung (Emperor und Anakin) zeigt, dass das vorgeschlagene Paar 13 und 23 Nachbarn hat. Multipliziert ergibt das einen Wert von 253, was auch Networkx berechnet hat. Bei der fünften Empfehlung (Obi-Wan und Emperor) beträgt der Koeffizient bereits nur noch 143.\n",
    "\n",
    "## 6.3 Resource Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_characters(lst_prediction_methods, lst_predictions, 2, 7, 30)\n",
    "\n",
    "print()\n",
    "print(\"Untersuchung einzelner Top Predictions:\")\n",
    "print()\n",
    "print(\"Gemeinsame Nachbarn von Obi-Wan und Rabe:\")\n",
    "print(sorted(nx.common_neighbors(graph_first_episode, 5, 33)))\n",
    "print(\"Anzahl der Nachbarn dieser Nachbarn:\", graph_first_episode.degree(sorted(nx.common_neighbors(graph_first_episode, 5, 33))))\n",
    "print(\"Inverse dieser Anzahl:\", [round(1 / graph_first_episode.degree(neighbor), 5) for neighbor in sorted(nx.common_neighbors(graph_first_episode, 5, 33)) if graph_first_episode.degree(neighbor) > 0])\n",
    "print(\"Summe der Inversen:\", round(sum(1 / graph_first_episode.degree(neighbor) for neighbor in sorted(nx.common_neighbors(graph_first_episode, 5, 33)) if graph_first_episode.degree(neighbor) > 0), 3))\n",
    "print()\n",
    "print(\"Gemeinsame Nachbarn von TC-14 und Tey How:\")\n",
    "print(sorted(nx.common_neighbors(graph_first_episode, 4, 8)))\n",
    "print(\"Anzahl der Nachbarn dieser Nachbarn:\", graph_first_episode.degree(sorted(nx.common_neighbors(graph_first_episode, 4, 8))))\n",
    "print(\"Inverse dieser Anzahl:\", [round(1 / graph_first_episode.degree(neighbor), 5) for neighbor in sorted(nx.common_neighbors(graph_first_episode, 4, 8)) if graph_first_episode.degree(neighbor) > 0])\n",
    "print(\"Summe der Inversen:\", round(sum(1 / graph_first_episode.degree(neighbor) for neighbor in sorted(nx.common_neighbors(graph_first_episode, 4, 8)) if graph_first_episode.degree(neighbor) > 0), 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obi-Wan wird mit sechs Nennungen am häufigsten in den Top 30 Empfehlungen genannt. Eine Bestätigung der Top-Charakteren lässt sich nur schwer begründen. Zwar haben einige der Top Charakteren, wie Obi-Wan und Jar Jar, viele Nachbarn, doch hängt es bei dieser Methode auch davon ab, wieviele Nachbarn die gemeinsamen Nachbarn haben.\n",
    "\n",
    "Bei der Analyse der obenstehenden Top 5 Empfehlungen fällt auf, dass bei der ersten Empfehlungen (Obi-Wan und Rabe) deren gemeinsamer Nachbarn (Qui-Gon, Anakin, Mace Windu, Ki-Adi-Mundi und Yoda) teilweise viele Nachbarn haben. Dank der Nachberechunung können wir aufzeigen, dass dank der drei Nachbarn mit je nur sechs Nachbarn (Inverse 0.167) der Koeffizient mit 0.582 relativ hoch ist. Zum Vergleich hat die fünften Empfehlung (TC-14 und Tey How) nur zwei gemeinsame Nachbarn und trotzdem einen Koeffizienten von 0.433, weil einer der Nachbarn nur drei Nachbarn hat. Entsprechend wird es trotz nur zweier Nachbarn als Wahrscheinlich erachtet, dass sich TC-14 und Tey How kennen.\n",
    "\n",
    "Zusammengefasst wurde unsere Erwartung, dass unterschiedliche Berechnungsmethoden unterschiedliche Resultate liefern, bestätigt. Die Wahl der Methode ist also zentral für eine gute Prediction. Da in Star Wars die Charakteren, deren Ziel, Aufgaben, Herkunft und Weltansichten zum Teil sehr unterschiedlich sind, würden wir aus unserer drei Methoden Jaccard Coefficient verwenden. Da innerhalb von Communities in der Regel viele Verbindungen bestehen, würden so z.B. innerhalb des Jedi-Ordens neue Kontakte vorgeschlagen. Denkbar wäre auch eine Metrik, die entweder Communities (z.B. Soundarajan Hopcroft) oder Kontakte zu wichtigen Charakteren stärker berücksichtigt."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Hypothesentest\n",
    "\n",
    "Wir untersuchen, ob der Degree der Charakteren mit der Anzahl Szenen, in denen sie vorkommen, zusammenhängt. Wir führen dazu einen monadischen Hypothesentest durch, die Hypothese lautet: \"In der ersten Episode gibt es einen Zusammenhang zwischen dem Degree von Charakteren und der Anzahl Szenen, in denen sie ersichtlich sind.\" Wir definieren dazu eine Irrtumswahrscheinlichkeit von 0.05. Ist der p-Wert tiefer, bestätigen wir unsere Hypothese, ansonsten gibt es keinen Zusammenhang.\n",
    "\n",
    "Im ersten Schritt berechnen wir den Degree und extrahieren die Anzahl Szenen, in denen die Charakteren vorkommen. Die Anzahl Szenen ist bereits im Datensatz von Kaggle enthalten. Danach berechnen wir die originale Pearson Korrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(graph_first_episode.degree())\n",
    "values = nx.get_node_attributes(graph_first_episode, 'size')\n",
    "\n",
    "degree_list = [degrees[node] for node in graph_first_episode.nodes()]\n",
    "value_list = [values[node] for node in graph_first_episode.nodes()]\n",
    "\n",
    "original_correlation, _ = pearsonr(degree_list, value_list)\n",
    "print(\"Die originale Korrelation beträgt:\", round(original_correlation,3))\n",
    "\n",
    "plt.scatter(value_list, degree_list, color='green')\n",
    "plt.title('Scatterplot von Degree und Size')\n",
    "plt.ylabel('Grad der Vernetzung')\n",
    "plt.xlabel('Anzahl der Szenen')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Pearson Korrelation zwischen dem Degree und der Anzahl Szenen ist mit 0.89 relativ, es gibt also einen Zusammenhang zwischen diesen beiden Variablen. Im Scatterplot wird allerdings ersichtlich, dass viele Charakteren in weniger als 10 Szenen auftauchen, der Degree aber von 1 bis 10 reicht. In der Praxis müsste man die Korrelation genauer untersuchen (Stichwort Scheinkorrelation und Ausreisser). Um die Funktionsweise des Hypothesentests aufzuzeigen, gehen wir hier davon aus, dass die Korrelation korrekt ist.\n",
    "\n",
    "Nun führen wir die Permutation durch. Wir führen dazu 50'000 Permutationen durch und vermischen dabei immer die Werte in beiden Vektoren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_permutations = 50_000\n",
    "significance_level = 0.05\n",
    "count_greater = 0\n",
    "\n",
    "lst_new_correlations = []\n",
    "\n",
    "for _ in range(num_permutations):\n",
    "    np.random.shuffle(value_list)\n",
    "    np.random.shuffle(degree_list)\n",
    "    permuted_correlation, _ = pearsonr(degree_list, value_list)\n",
    "    lst_new_correlations.append(permuted_correlation)\n",
    "    if permuted_correlation >= original_correlation:\n",
    "        count_greater += 1\n",
    "\n",
    "print(\"Anzahl neuer Korrelationen, die grösser sind:\", count_greater)\n",
    "\n",
    "p_value = count_greater / num_permutations\n",
    "\n",
    "print(\"Der p-Wert beträgt:\", p_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den 50'000 Permutationen ist keine Korrelatioen höher als der originale Wert von 0.893. Das ist zwar eher überraschend, doch gemäss Definition können wir unsere Hypothese bestätigten, es gibt einen signifikanten Zusammenhang zwischen der Anzahl Szenen, in denen ein Charakter vorkommt und dessen Degree.\n",
    "\n",
    "Aus Interesse plotten wir nun noch ein Histogramm, das die Verteilung der berechnete neuen Korrelationen aufzeigt und sie mit dem originalen Wert vergleicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lst_new_correlations, bins=50, color='blue', alpha=0.7)\n",
    "plt.axvline(x=original_correlation, color='red', linestyle='dashed', linewidth=2)\n",
    "plt.title('Histogramm der permutierten Korrelationen')\n",
    "plt.xlabel('Korrelationskoeffizient')\n",
    "plt.ylabel('Häufigkeit')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Histogramm bestätigt, dass der originale Korrelationskoeffizient deutlich höher als die neuen, aus der Permutation berechneten, Werte liegt und bestätigt visuell die Annahme der Hypothese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Ausblick\n",
    "\n",
    "Die Soziale Netzwerkanalyse unserer Starwars Krieger fanden wir sehr spannend und interessant. Wir haben viele neue Erkenntnisse gewonnen und konnten auch unsere erlernten Kenntnisse aus dem Modul Soziale Netzwerkanalyse anwenden. Wir haben uns sehr gefreut, dass wir die Möglichkeit hatten, dieses Projekt zu bearbeiten und möchten uns an dieser Stelle bei unseren Dozenten Michael Henninger bedanken. \n",
    "\n",
    "Durchaus hätte man noch weitere interessante Analyse durchführen können wie bsp. das Filtern nach bestimmten Species oder Attribute von Starwars Charakteren, wie wir dies im Kapitel 4, für die Entwicklung der Actor Zentralitätswerten versucht haben. Leider sind wir in dieser hinsicht jedoch begrenzt, da der Datensatz von Kaggle nicht alle Starwars Charaktere beinhaltet. Eine andere weitere spannende Analyse wäre noch die Gesprächsanalyse Bechel Test, welche uns die Gleichberechtigung der Geschlechter in den Starwars Filmen aufzeigt. Was aber widerum schwierig, ist da es unterschiedliche Geschlechter gibt und wir diese nicht einfach in zwei Kategorien aufteilen können, es wäre somit ein erweiterter Bechel Test spezifisch für Starwars.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_Learning-EdiEON_k",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e05621076ca54fd44c7c0ce1b6a0f390ce58f91d2fe1d180f4996db114adea7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
